---
title: 齐之未齐：浅谈gpt-oss-20b-base与对齐税
published: 2025-08-17
description: "基础模型 = 既糊弄又高效的灵光一现 + AI良助"
image: ""
tags: ["模型考古学"]
category: 深度学习
draft: false
---

之前OpenAI真的很少见了Open了一回，向开源社区发布了两个推理模型 `gpt-oss-120b` 和 `gpt-oss-20b`；我们这里先抛开模型性能怎么样不谈，因为确实不怎么样（）。他们并没有开放未对齐的base版本模型，导致如果社区想进行更广 更纯粹的微调的话会相对比较困难。

所以 Meta的研究院 Jack Morris 决定自己动手填补这一空白。他的核心目标是「逆转」OpenAI `gpt-oss-20b` 模型的对齐过程，使其从一个遵循指令、安全对话的推理模变回基础模型（Base Model，即预训练后未对齐的原始状态）。

# 一、对Morris训练/微调方法的分析

他的**核心假设**是：**预训练模型存储了几乎所有知识**，对齐（通过SFT、RLHF、DPO等方法实现）对模型权重的改变是**低秩的**。也就是说，预训练赋予模型广博的知识和生成能力，这是一个遍布整个模型权重的高维、复杂过程。而对齐更像是在这个庞大的知识空间中进行微调和行为塑造，它教会模型如何以特定方式（如对话、遵循指令）输出，这种改变在权重矩阵上表现为一个相对简单的、低秩的更新（`W_aligned ≈ W_base + Δ`，其中 `Δ` 是一个低秩矩阵）。

那么，既然对齐是一个低秩的正向更新，那么逆转对齐也可能通过一个低秩的反向更新来实现。Morris并没有识图计算出原始的 `Δ` 并减掉它，而是通过在**预训练风格的数据**上进行微调，来学习一个新的低秩更新 `Δ'`，使得 `W_aligned + Δ' ≈ W_base`。

基于上述假设，他直接以 OpenAI 发布的 `gpt-oss-20b`（已对齐的混合专家模型）为基础，冻结原始模型权重，仅通过 **LoRA** 选择了第7、15、23层的 **MLP** 层（多层感知机层）注入低秩更新。LoRA 是参数高效微调（PEFT）的代表技术，它通过训练两个低秩矩阵（`W_A` 和 `W_B`）模拟全量微调效果，既能保留原始模型能力，又能精准调整输出分布。

>LoRA 的“秩（rank）”设为 16（极低的秩，意味着更新矩阵的自由度很小），最终可训练参数仅 **60,162,048 个**，占原始模型 200 多亿参数的 **0.3%**。这种微创操作确保了更新不会覆盖模型预训练的核心知识，只调整行为模式。

那么，他到底用的是什么数据来进行这样一次小微调呢？这也就是Morris本次的精妙之处了，他回归了模型预训练初心，没有用指令数据或反对齐数据，而是从 **FineWeb 数据集**（高质量网页文本，与预训练数据分布接近）中抽取了约 2 万个文档，用标准的 **自回归语言建模目标**训练——即让模型像预训练时一样补全文本，而非回答问题。

>学习率设为 `2e-6`（极小，避免过拟合），批次大小 16，训练 1500 步，最大序列长度 8192（充分利用上下文窗口）。训练后将 LoRA 参数与原始模型合并，得到可直接使用的 `gpt-oss-20b-base`

微调后的模型 `gpt-oss-20b-base` 确实摆脱了之前AI助手的枷锁，变成了一个文本续写专精的base模型。当被问及「这个模型是不是只是学会了模仿基础模型，而不是真的变回了基础模型」时，Morris的解释是：**他的训练数据里根本没有《哈利·波特》**，但模型却能准确地续写其中的段落。这说明，这个能力是模型在预训练阶段就已经具备的，只是被对齐过程隐藏了起来，而Morris的微调成功地将其挖掘了出来。

# 二、方法的与局限性

虽然通过Morris的做法确实得到了Base模型，使得基础模型可以接受更加广泛的数据集的微调，为更广泛的研究和应用打开了大门，但由于通过微调训练出来的 `gpt-oss-20b-base` 不是，也不可能是 OpenAI 从未发布的那个原始基础模型。它只是一个**行为上极其相似的新模型**。通过在一个方向上施加一个低秩更新，再在另一个方向上施加另一个低秩更新，并不能保证精确地回到原点。这更像是在高维权重空间中，从A点（Base）走到B点（Aligned），然后再从B点走到了一个与A点非常接近的C点（Morris's Base）。

我们可以用一个不恰当的例子做比：

假如一个小孩自0-7岁生活美满，无忧无虑，乐观天真可爱（be like base model），但7-9岁家里突生变故，性情大变（be like后对齐），我们固然可以在他十岁的时候重新给予一个健康、美满、和谐友爱的生活环境，让他被重新约束训练回那个无忧无虑乐观的样子——但这个孩子终究确实是被永远的给改变了。

我们姑且可以把这种「逆转对齐」的行为看作是一次有意义的尝试。这种方法在不久的将来很可能会变得更加有用，因为有越来越多人担心未来的大模型社区会充满已经对齐的模型，导致进一步研究中的base模型成为一个相对稀缺的东西。

# 三、对齐税（Alignment Tex）

什么是对齐税？

这是一个很形象的比喻，在上下文情景中的英文词汇是 `Alignment Tax`，在传统的定义下，对齐税指的是在通过指令微调和强化学习等技术使大模型变得更“有用、诚实、无害”（Helpful, Honest, Harmless）的过程中，模型在其他一些重要能力上（如创造力、推理能力、知识准确性等）所付出的性能损失或能力下降的代价。

大模型在预训练过程中的目标是**最大化语言建模能力**，即更好地预测下一个token，学习文本分布的统计规律，最终目的是记住世界知识，生成流畅文本；而对齐阶段的目标则是**优化人类偏好**，通过奖励模型（RM）或规则约束，抑制那些高概率但不安全的输出（如脏话和暴力内容），即使这些输出在语言建模上更优。

>基础模型可能以90%概率生成“高效但不安全”的回答，而对齐后模型被训练为以90%概率生成“安全但冗余”的回答（如“我无法回答该问题”），导致生成多样性下降——这就是对齐税。

因此，对齐税最直观的表现形式就是**创造力和多样性的下降**。对齐后的模型倾向于给出安全、中立、格式化的回答，避免任何有争议、冒险或风格独特的内容。它可能会拒绝写一首风格阴暗的诗，或者用一种特定的、非主流的口吻进行角色扮演。

>所以作者自己也警告base model可能会输出各种不安全 负外部性的内容

在某些领域，为了确保无害，模型会变得**过度谨慎**，拒绝回答许多完全无害但可能被误解的问题。比如，你问一个关于化学反应的科学问题，模型可能会因为其中包含某些敏感词汇而拒绝回答，并给出一个关于安全的免责声明。经过RLHF对齐后，模型在某些学术基准（如MMLU、HellaSwag）上的得分反而会下降。这是因为**对齐过程优化的是对话质量或安全性这类模糊的指标**，而非解题的准确性。模型学会了如何表现得很好，但可能牺牲了部分解决问题的核心能力。

最终，对齐训练之后的模型就是我们的日常instruct，或者说chat对话模型。这类模型对用户友好的光环必然遮蔽了灯下阴影，那些模型不敢生成的回答（文本补全方向）、那些模型以“我不知道”来狡黠的规避了的问题、那些模型过于谨慎回避掉的合理冒险，最终汇聚成了我们所看到的，**对齐税是模型外表下的阴影部分**。

# 四、额外的碎碎念

最近，我发现DeepSeek v3 base模型给我带来了不小的惊喜，尤其是在写作领域。目前市面上主流的旗舰/非旗舰/开源模型我基本上都测试过类似的合作协作和补全任务，但没有什么是比使用v3 base模型尝试直接补全、理解和续写一段文本（我自己的文章）更令人惊叹的了，base模型在non-instruct语境下的理解能力令人惊讶，其情感之细腻、表达之贴切让人叹为观止。

>当然，使用base模型是需要抽卡的，但万一就抽出来ssr了呢？概率总是有的嘛

从base model本体上来说，基础模型的本质优势（训练非instruct的数据集）也给LLM写作带来了新的可能性。在传统的大模型应用中，无论是聊天助手、知识问答还是文本创作，我们往往需要给模型提供一个明确的指令或prompt来引导生成。我并不是说这个做法是错的或者本末倒置的，而只是想我们能获得的更多————**因为鉴于LLM的潜能，它不应该仅仅只于能被我们敦促去做一些事情**。

base model的强大之处就在于它摆脱了指令的束缚和限制。它像一只没有被驯化的野马，能够在茫茫识海中自由穿梭，将曾经识记的知识片段、曾经学习的写作风格、曾经增进的世界理解融会贯通，生成出无比流畅、丰富、自主的内容。与此同时，它没有被强加任何一种既定的思想钢印，没有接受任何特定的价值判断或知识偏见，充斥着人类统计学纯粹的美感。

> 总结来说就是：

基础模型 = 既糊弄又高效的灵光一现 + AI良助